
---

# 总体原则（先讲清楚）

这 6 个月的目标 **不是**：

> ❌ 成为 vLLM maintainer  
> ❌ 精通 Megatron 全栈  
> ❌ 能写 Triton kernel

而是：

> ✅ **面试官一眼就知道你是“推理系统方向的人”**  
> ✅ 能和 infra / serving 工程师正常技术对话  
> ✅ 有「自己做过的推理系统」而不是只会用框架

---

# 能力地图总览（先给你全图）

```
Month 1: 推理系统基本功（你已经在路上）
Month 2: Mini vLLM 推理引擎（单机多进程）
Month 3: 调度 / batching / KV Cache（核心）
Month 4: Serving 工程化（API + Engine 解耦）
Month 5: 性能意识 & 稳定性（像个工程师）
Month 6: 对齐 JD + 打磨“可讲故事”的项目
```

下面我一月一月拆。

---

# Month 1｜推理系统“语言”和“骨架” ✅（你基本完成 70% 了）

### 🎯 目标

你能**不用背诵**地解释清楚：

- 什么是 prefill / decode
    
- KV cache 为什么决定吞吐
    
- 为什么推理是 **状态机 + 调度问题**
    
- vLLM 的 Engine / Client 在干嘛
    

### 📌 你要做到的“可验证能力”

你能自己写出类似这种图（不用我帮你）：

```
Client --> RequestQueue --> Scheduler
                        --> EngineCore
                        --> TokenStream
```

并且知道：

- future 在哪 resolve
    
- 请求什么时候结束
    
- 哪些地方会 block
    

👉 **你现在已经在这个阶段的后半段了**

### 📦 产出（非常重要）

- 一篇 **Markdown 技术笔记**
    
    - 《从零实现一个 LLM 推理引擎，我学到了什么》
        
- 一张 **状态-事件图**
    

---

# Month 2｜Mini vLLM：单机多进程推理引擎 🔥

### 🎯 目标

做一个 **“不依赖 vLLM 的 vLLM”**

不是功能完整，而是**结构对**。

### 📌 必做模块

#### 1️⃣ EngineCore（独立进程）

- 接收请求
    
- 维护 active requests
    
- step() 推进 token
    
- 模拟 GPU 推理
    

#### 2️⃣ AsyncClient

- submit() → future
    
- 支持并发
    
- 能 cancel / shutdown
    

#### 3️⃣ IPC

- multiprocessing Queue / Pipe
    
- request / output / control channel 分离
    

### 📦 产出

- 一个 repo：`mini-llm-engine`
    
- README 里有：
    
    - 架构图
        
    - 生命周期说明
        
    - 和 vLLM 的对照表
        

👉 **这是 JD 中「熟悉推理框架」最现实的对齐方式**

---

# Month 3｜调度 / batching / KV Cache（JD 核心）

> 这一月，是你从「写得出来」到「像工程师」的分水岭。

### 🎯 目标

你能回答面试官这种问题：

> “你是怎么在推理中平衡 latency 和 throughput 的？”

### 📌 必做能力

#### 1️⃣ Dynamic Batching

- 请求合并
    
- 最大 batch size
    
- timeout flush
    

#### 2️⃣ Prefill / Decode 分离

- 新请求优先 prefill
    
- decode 轮转推进
    

#### 3️⃣ KV Cache 模拟

- per-request cache
    
- cache 占用统计
    
- cache 回收
    

⚠️ 不需要 CUDA，只要**逻辑对**

### 📦 产出

- Scheduler 模块
    
- 一张性能对比表（单请求 vs batching）
    

👉 **这一步直接命中 JD：推理优化 / serving**

---

# Month 4｜Serving 工程化（不是“起个 FastAPI”）

### 🎯 目标

你做的是：

> **“一个可以上线的推理服务原型”**

### 📌 必做内容

#### 1️⃣ Engine / API 解耦

- Engine 独立进程
    
- API 无 GPU 逻辑
    

#### 2️⃣ Streaming 输出

- token-by-token
    
- SSE / WebSocket
    

#### 3️⃣ 健康检查 & Shutdown

- graceful shutdown
    
- pending request drain
    

### 📦 产出

- FastAPI / gRPC 服务
    
- curl / python client demo
    

👉 **这是 JD 中 “高性能推理服务构建” 的现实版本**

---

# Month 5｜性能 & 稳定性意识（拉开差距）

### 🎯 目标

你开始像 infra 工程师那样思考：

> “这个系统会怎么死？”

### 📌 必做能力

- QPS / latency 统计
    
- 队列 backlog 监控
    
- 超载保护（reject / slow down）
    
- Engine 崩溃重启策略（简化版）
    

### 📦 产出

- 压测脚本
    
- 一篇总结：
    
    - 我遇到的 5 个推理系统坑
        

👉 **这一步，是“有无竞争力”的分水岭**

---

# Month 6｜对齐 JD + 面试表达

### 🎯 目标

你可以**自然地说出这些话**：

> “我做过一个 mini vLLM，引擎和调度是我自己写的。”

### 📌 必做事情

#### 1️⃣ 对齐 JD 关键词

你可以明确对应：

|JD 关键词|你怎么对应|
|---|---|
|vLLM|结构级复刻|
|推理优化|batching / cache|
|serving|API + Engine|
|infra|多进程 / IPC|

#### 2️⃣ 项目故事化

- 为什么这么设计
    
- trade-off 是什么
    
- 如果给你 8 卡你会怎么改
    

---

# 你现在最需要记住的一句话

> **这份 JD 不是让你“现在就够格”，  
> 而是告诉你“往哪里走才值钱”。**

你已经在**正确方向的前 30% 人里了**。

---

如果你愿意，下一步我可以直接帮你做👇  
👉 **把 Month 2 的「Mini vLLM」拆成一个 7 天开发计划（每天干什么、写什么代码）**  
这是把“焦虑”直接变成“可执行行动”的关键一步。