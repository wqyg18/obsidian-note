**问题 2：当一个大模型处理请求时，分为“首字生成”和“后续字生成”两个阶段。请尝试解释：**

1. **Prefill（预填充）**：模型在处理你输入的那一大串 Prompt 时，是在做什么？
    
2. **Decode（解码）**：模型在生成第 2 个字、第 3 个字时，计算模式和第一个字有什么不同？


---

**问题 3：KV Cache 到底存的是什么？在 Transformer 的 $Attention(Q, K, V)$ 公式中，为什么我们只需要存 K 和 V，而不需要存 Q (Query)？**

---

## 📘 大模型推理性能优化：核心问答汇总

### 1. 基础架构：推理 vs 训练

- **问题**：推理和训练在显存占用上最大的区别是什么？
    
- **回答**：训练需要保存**梯度**和**激活值**（用于反向传播），显存消耗是静态且巨大的；推理不需要反向传播，其显存压力主要来自**模型权重**和动态增长的 **KV Cache**，且由于输入输出长度不可控，显存需求具有高度的不确定性。
    

### 2. 过程拆解：Prefill vs Decode

- **问题**：为什么模型生成第一个字快，后面一个一个出字时感觉“节奏”变了？
    
- **回答**：
    
    - **Prefill**：处理输入 Prompt，是**并行计算**，属于**计算密集型（Compute-bound）**，GPU 利用率高。
        
    - **Decode**：自回归生成，每步只进 1 个 Token，必须等前一个词算完才能算下一个，属于**访存密集型（Memory-bound）**。
        

### 3. 技术核心：KV Cache

- **问题**：什么是 KV Cache？为什么它必须存在显存（VRAM）里？
    
- **回答**：KV Cache 存储了过去所有 Token 的 Key 和 Value 向量，避免每生成一个新词都要重复计算旧词的 Attention。它必须存放在显存中，因为 Decode 阶段需要极高的**内存带宽**（1TB/s 以上）来频繁读取数据，普通内存（RAM）太慢，会导致 GPU 严重空转。
    

### 4. 性能瓶颈：为什么 Decode 是 Memory-bound？

- **问题**：既然计算量变小了，为什么 Decode 反而成了瓶颈？
    
- **回答**：因为在 Decode 阶段，GPU 搬运“模型权重”和“KV Cache”到计算核心的时间，远长于实际进行矩阵运算的时间。GPU 强大的算力在“等”数据搬运，速度受限于**显存带宽**。
    

### 5. 吞吐量关键：Batch Size

- **问题**：为什么 Batch Size 越大，推理系统的吞吐量（Throughput）越高？
    
- **回答**：因为模型权重（Weights）是所有请求共享的。当 Batch 增大时，GPU 搬运一次权重可以同时为多个用户服务，**分摊了访存开销**，提高了 GPU 核心的利用率。
    

---

## 🏆 终极目标：为什么 vLLM 比 Naive Batch 快？

我们可以用一张对比表来做最后的口语化陈述：

|**维度**|**Naive Batching (传统方式)**|**vLLM (PagedAttention)**|
|---|---|---|
|**内存分配**|**连续分配**：预留最大长度的“大槽位”|**分页分配**：像 OS 映射一样，碎片化存储|
|**显存浪费**|**严重**：存在内部碎片、外部碎片和预留浪费|**极低**：按需分配，浪费率降至 4% 以下|
|**并发能力**|**低**：显存很快被空槽位占满 (OOM)|**高**：同样的显存能支持多出数倍的 Batch Size|
|**性能结果**|吞吐量低，GPU 带宽利用不充分|**吞吐量提升 2-4 倍**，彻底压榨 GPU 带宽|

> 总结一句话：
> 
> vLLM 借鉴了操作系统的虚拟内存思想，通过 PagedAttention 解决了显存碎片化问题，使得在同样的显存下能跑更大的 Batch Size，从而通过提高并行度，从底层解决了 Decode 阶段 Memory-bound 的瓶颈。

---
