🧠 笔记 1：生成模型的目标 —— 学习 p(x)

在无条件生成中，生成模型的目标是学习一个 **概率密度函数** p(x)，它描述了“什么样的图像是自然的”。

x 是一张图像（高维向量），p(x) 越大表示图像越真实、越可能来自训练数据。

模型不会输出“图像的概率”，而是学会如何从这个分布中 **采样出新的图像**。

🎲 笔记 2：什么是“从 p(x) 中采样”？

采样就是：从模型学到的图像分布中随机“挑选”一张图像。

在扩散模型中：

从高斯噪声开始：x_T ~ N(0, I)

逐步去噪 → x_{T-1}, x_{T-2}, ..., x_0

最终得到 x_0 ~ p(x)：这就是生成的图像。

每次采样结果可能不同，但都“像真的”。

📌 笔记 3：概率密度 vs 概率的区别（非常重要）

| 概念     | 含义                                           | 举例                                      |
|----------|------------------------------------------------|-------------------------------------------|
| 概率密度 p(x) | 表示某个具体图像在图像空间中“落点的密度”，用于连续变量 | p(x) 越大表示越自然                       |
| 概率     | 某个区域内所有图像的总可能性（积分）          | Prob(x ∈ A) = ∫_A p(x) dx                 |

✅ 提醒：在连续空间中，具体图像的“概率”永远是 0，但可以有密度。

🧾 总结一句话 ✅

扩散模型的本质是：**学习数据的概率密度函数 p(x)**，然后通过一个逐步去噪的采样过程，从中“生”出图像。
***
🧠 目标是什么？  
在扩散模型中，我们希望生成“有条件”的图像，比如：

生成一张“猫”的图片（条件就是“猫”）

或者生成某个类别的图像，比如标签是 7 的数字图像

这时，我们想要的生成结果是满足这个条件的，也就是我们希望模型学的是：

$$
p(x \mid c)
$$

意思是：“在已知类别 $c$ 的情况下，生成图片 $x$ 的概率”。

---

🧱 什么是 CG（Classifier Guidance）？  
CG 的核心思想是：

让无条件扩散模型在生成时，被一个外部的分类器引导，引导它生成你想要的类别。

这个分类器 $p_\phi(c \mid x)$ 是你另外训练好的，给定一张图片 $x$，它能预测这张图片属于哪个类别 $c$。

---

📐 数学公式解释  
我们希望最大化条件概率 $p(x \mid c)$，但我们只有一个无条件的扩散模型，它学的是 $p(x)$。

我们可以用贝叶斯公式把条件概率写出来：

$$
p(x \mid c) \propto p(c \mid x) \cdot p(x)
$$

注意这里的 $\propto$ 表示“成比例”，我们省略了分母 $p(c)$，因为它跟 $x$ 无关。

意思是：

你想生成满足 $x \mid c$ 的图，

但你现在有：

- 一个原始扩散模型，知道 $p(x)$  
- 一个分类器，知道 $p(c \mid x)$  

于是你可以用这两个凑出你想要的目标：$p(x \mid c)$

---

🌀 CG 的关键：在扩散采样时做引导  
在扩散模型中，采样过程中我们不断 denoise（反扩散），每一步都预测图像该往哪个方向调整。

为了让模型生成的图像朝着类别 $c$ 更像，我们在每一步中加一个“引导项”，这个引导项就是分类器告诉我们：这张图如果想更像类别 $c$，应该怎么调整！

这个引导项怎么加呢？

---

🧮 具体公式  
Denosing Diffusion Implicit Model (DDIM) 或 Score-based Diffusion Model 都涉及一个关键操作：

采样时我们使用的是图像的“梯度”或“方向”

所以可以在每一步加入一个梯度偏移，来控制生成方向。

最终的采样公式大概是：

$$
\nabla_{x_t} \log p(x_t \mid c) = \nabla_{x_t} \log p(x_t) + w \cdot \nabla_{x_t} \log p(c \mid x_t)
$$

解释一下：

- $x_t$ 是当前的图像（还带有噪声）  
- 第一项：是扩散模型本身的梯度，告诉我们怎么去 denoise（去噪）  
- 第二项：是分类器的梯度，告诉我们怎么让图像更像类 $c$  
- $w$：是你设定的权重，调节引导强度  

🟡 所以你加上了分类器引导以后，生成出来的图像就更符合类别 $c$ 了！

---

🎯 举个例子（直观理解）  
想象你让模型画一张图：

原来的扩散模型在画画，它没啥目标，只是“把噪声变成像图像的东西”（学的是 $p(x)$）。

你有个分类器，它看着每一幅草图说：“哎，这不像猫，得改改耳朵”、“尾巴得长点！”

于是你每画一笔，分类器都来指导你方向：朝着“猫”这个类别更靠近。

这样，最后你就画出了一只猫！

---

✅ 总结一句话  
Classifier Guidance 的核心是：

用一个分类器的输出 $p(c \mid x)$ 的梯度，在扩散采样过程中引导无条件模型生成你想要的类别，构造出 $p(x \mid c)$。
